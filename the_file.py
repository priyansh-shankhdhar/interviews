# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PS-iZQLRvVIYnILmJ4Xd8u3mVqejnN9x
"""

import numpy as np
import pandas as pd
import regex as re
from google.colab import files
import os
import zipfile
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
from gensim.models import Word2Vec
nltk.download('punkt')
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder 
from sklearn.compose import ColumnTransformer

class wordy:
  def __init__(self,data):
    self.model=None
    self.data=data

  def create_word_to_vec(self):

    if self.model is None:
      print('word to vec forming: ')

      data=self.data #pd.read_csv('/content/more/marketing_sample_for_naukri_com-jobs__20190701_20190830__30k_data.csv')

      wv=data[['Job Title','Key Skills','Role Category','Functional Area','Industry','Role']]

      wv=wv.fillna('')
      
      wv['Key Skills']=wv['Key Skills'].apply(lambda x: str(x).replace('|',''))
      
      wv['Functional Area']=wv['Functional Area'].apply(lambda x: x.replace(',',''))
      
      wv['Industry']=wv['Industry'].apply(lambda x: x.replace(',',''))

      print('==',end='>')

      arr=[]
      for i in range(len(wv)):
        x=(wv.iloc[i][0]+' '+wv.iloc[i][1]+' '+wv.iloc[i][2]+' '+wv.iloc[i][3]+' '+wv.iloc[i][4]+' '+wv.iloc[i][5]).lower()
        x=re.sub('\[[0-9]"\]',' ',x)
        x=re.sub('\d',' ',x)
        x=re.sub('\s+',' ',x)
        arr.append(x)
      print('==',end='>')
      sentences=[nltk.word_tokenize(i) for i in arr]
      print('==',end='>')
      for i in range(len(sentences)):
        sentences[i]=[word for word in sentences[i] if word not in stopwords.words('english')]
      print('==',end='>')
      self.model=Word2Vec(sentences,min_count=1,size=1)
      ########################################################### CAN BE DELETED JUST FOR SHOW--down
      print('=>\ncompleted')

    else:
      #print('word to vector has already formed')
      return True

  def get_vec(self,word=''):
    if self.model is None:
      return ('model not formed please call: wordy.create_word_to_vec() ')    
    try:
      x=self.model.wv[word]
      return x[0]
    except:
      return 0.5

  def get_model(self):
    if self.model:
      return self.model
    else:
      print('model not formed please call: wordy.create_word_to_vec()')

def filter_data(df,w,city,main):
  try:
    df=df.drop(['Job location lat','Job starting date','Job location long','generated post id','searchkey','image_url','user id','job_id','Company name','Apply by date','timestamp','Hiring for month'],axis=1)
  except:
    pass
  if main:
    df=df.iloc[:-2]
  #########################################################################
  df['Salary per Month']=df['Salary per Month'].apply(lambda x: float(str(x).replace('k','000').partition('-')[0])/1000 )

  ########################################################################
  def ch(x):
    x=x.split('-')
    x="".join(x)
    x=re.sub('\D',"",x)
    return x

  df['Year of experience']=df['Year of experience'].apply(lambda x: float(ch(x)))

  #########################################################################
  d={ 'open':1.0, 'close': 0.0 }
  df=df.replace({'status': d})

  #################################################

  df['Qualification']=df['Qualification'].apply(lambda x: float(x.count(','))+1 )

  ###############################################################################################

  #clean the city.csv file 
  city=city.drop(['State','Type','Population class','Type'],axis=1)
  city['Name of City']=city['Name of City'].apply(lambda x: x.lower())
  city['Population (2011)']=city['Population (2011)'].apply(lambda x: float(re.sub(',','',x)))
  # assinging value sto each city w.r.t the population of the city and storing in  a dictionary
  k=[]
  dic={}
  cit_lis=[]
  for i in range(len(city)):
    v=city.iloc[i][1]
    if v >= 10000:
      m= 4
    if v >= 20000:
      m=3
    if v >= 50000:
      m=2
    if v >= 100000:
      m=1
    else :
      m=5
    cit_lis.append(city.iloc[i][0])
    dic.update({city.iloc[i][0] : m })

  def jo(x):
    x=x.split(',')
    sum=0
    t=[]
    for i in x:
      p=i.split('(')[0].strip(' ')
      t.append(p)
    for i in t:
      for j in i.split():
        j=j.lower()
        if j in cit_lis:
          sum+=dic[j]
    if sum==0:
      return 10
  # here we are using this such that the value does no ecceed or decrease a limit we want it to be in a reage
  # the equation of given fuction below can be seen here-> https://www.google.com/search?rlz=1C1CHBD_enIN894IN895&sxsrf=ALeKk023pu9vQvanAQYWZ0dpLw-2Ug1QWw%3A1599882619675&ei=e0VcX_bhKPid4-EPj5KXyAs&q=y%3D20%2Fx&oq=y%3D20%2Fx&gs_lcp=CgZwc3ktYWIQAzICCAAyBAgAEB4yBAgAEB4yBAgAEB4yBAgAEB4yBAgAEB4yBAgAEB4yBAgAEB4yBggAEAUQHjIGCAAQBRAeOgcIIxDqAhAnOgQIABBDOggIABCxAxCDAToFCAAQsQM6BAgjECc6AggmOgYIABAKEB5QubafAliR2p8CYMPhnwJoAXAAeACAAecCiAGDCZIBBzAuNS4wLjGYAQCgAQGqAQdnd3Mtd2l6sAEKwAEB&sclient=psy-ab&ved=0ahUKEwi2i-mZ2-LrAhX4zjgGHQ_JBbkQ4dUDCA0&uact=5
    return (20/sum)-len(t)

  ################################################################
  df['Job location']=df['Job location'].apply(lambda x: jo(x))
  #############################################################
  # Now for job title and about job i have created a word to vec from a preexisting dataset from naukri.com
  #with that information once the word to vect for specific word was created (here ecah vecto in 1 dimesional)
  #we replace the word in 'job title' and 'about job' woth their specific vector and if vector is not available
  #the surely it means the work is not much in demand thus the vector we get is zer

  #so for multiole words in a block we take avergae of vectors of each word in block in do_it(x) funtion 

  #w=wordy() #load the wordy class # do not uncomment w=wordy()
  w.create_word_to_vec() #this creates the word to vector

  #this function averages out the vector value for each block
  def do_it(x):
    x=x.lower().split()
    c=0
    sum=0
    for j in x:
      c+=1
      sum+=w.get_vec(j)#here w.do(j) gets the vector for string j
    return sum/c 

  df['Job title']=df['Job title'].apply(lambda x: do_it(x))

  ###################################################################


  for i in range(len(df['About job'])):
    sentence=df['About job'][i]
    sentence=str(sentence.replace('  ','')).split(',')

    for i in range(len(sentence)):
      sentence[i]=sentence[i].replace('  ','').replace('.','').lower().split() 
      sentence[i]=[word for word in sentence[i] if word not in stopwords.words('english') ] 
      sentence[i]=' '.join(sentence[i]) 
      df['About job'][i]=sentence[i] 

  df['About job']=df['About job'].apply(lambda x: do_it(x))

  ####################################################
  def req(x):
    di={'they would have to stay near our plant':4 ,'no':0}
    x=x.lower()
    try:
      return di[x]
    except:
      return -2

  df['Requirements']=df['Requirements'].apply(lambda x: req(str(x)))

  columnTransformer = ColumnTransformer([('encoder', 
                                        OneHotEncoder(), 
                                        [1])], 
                                      remainder='passthrough')
  h=pd.DataFrame(columnTransformer.fit_transform(df), dtype = np.str) 

  return h

def unzip(path, dir_name):
  with zipfile.ZipFile(path, 'r') as zip_ref:
      os.mkdir(dir_name)
      j=%pwd
      zip_ref.extractall(os.path.join(j,dir_name))

def load_data():
  #data_to_load = files.upload()
  j=%pwd
  try:
    try:
      unzip(os.path.join(j,"data.zip"),'more')
    except:
      pass
    data=pd.read_csv(os.path.join(j,"more/marketing_sample_for_naukri_com-jobs__20190701_20190830__30k_data.csv"))
    df=pd.read_csv(os.path.join(j,'job.csv'))
    city=pd.read_csv(os.path.join(j,'city_lis.csv'))
    return data,df,city
  except:
    print("You Dont have the files get the files in same directory")


def wording(data):
  w=wordy(data)  
  w.create_word_to_vec()
  return w

def calls(h):
  #print(h)
  scalar = MinMaxScaler()
  scaled_data = scalar.fit_transform(h)
  km=KMeans(n_clusters=5,max_iter=2000,random_state=85)
  km=km.fit(scaled_data)
  #print(km.labels_)
  return km.labels_

#import_()
#data,df,city=load_data()
#w=wording(data)
#h=filter_data(df,w,city)
#op=calls(h)

class interview:
  def __init__(self):
    self.city=None
    self.w=None
    self.h=None

  def run(self):
    data,df,self.city = load_data()
    #print(data,df,self.city)
    if self.w is None:
      self.w=wording(data)
    self.h=df.loc[0:7]

  def pred(self,dframe):
    hh=self.h
    hh=hh.append(dframe,ignore_index=True)
    #print(hh)
    hh=filter_data(hh,self.w,self.city,main=False)
    l=calls(hh)
    ep=[]
    print(l)
    for i in range(len(dframe)):
      if l[8+i]==4 or l[8+i]==1:
        ep.append(np.array(df.iloc[i]))
    return pd.DataFrame(np.array(ep))